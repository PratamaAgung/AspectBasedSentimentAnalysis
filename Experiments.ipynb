{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formalizer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/formalizer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print(response)\n",
    "        return None\n",
    "    \n",
    "def stemmer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/stemmer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print(response)\n",
    "        return None\n",
    "    \n",
    "def tokenizer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/sentence/tokenizer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print(response)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('label/review1.csv')\n",
    "data2 = pd.read_csv('label/review5.csv')\n",
    "data3 = pd.read_csv('label/review1_2.csv')\n",
    "data4 = pd.read_csv('label/review2_2.csv')\n",
    "data5 = pd.read_csv('label/review3_2.csv')\n",
    "data6 = pd.read_csv('label/review5_2.csv')\n",
    "\n",
    "datal = [data1, data2, data3, data4, data5, data6]\n",
    "data = pd.concat(datal)\n",
    "data = data[['review', 'produk', 'packaging', 'pengiriman', 'general']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = []\n",
    "for d in data:\n",
    "    formalized_data = formalizer(d[0])\n",
    "    stemmed_data = stemmer(formalized_data)\n",
    "    preprocessed_data.append(stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d[1:] for d in data]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(preprocessed_data, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df= 1)\n",
    "X = vectorizer.fit_transform(preprocessed_data).toarray()\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "tokenized = [list(map(lambda x: vectorizer.vocabulary_.get(x), analyzer(line))) for line in preprocessed_data]\n",
    "\n",
    "y = [d[1:] for d in data]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(tokenized, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_words = 40\n",
    "train_x = sequence.pad_sequences(train_x, maxlen= max_words)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen= max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 10ms/step\n",
      "Accuracy report for produk: 0.609375\n",
      "64/64 [==============================] - 0s 326us/step\n",
      "Accuracy report for packaging: 0.625\n",
      "64/64 [==============================] - 0s 358us/step\n",
      "Accuracy report for pengiriman: 0.6875\n",
      "64/64 [==============================] - 0s 353us/step\n",
      "Accuracy report for general: 0.953125\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "embedding_size = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.vocabulary_), embedding_size, input_length= max_words))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(3, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    model.fit(train_x, to_categorical([t[i] for t in train_y], num_classes= 3), epochs= 16, verbose= 0)\n",
    "    scores = model.evaluate(test_x, to_categorical([t[i] for t in test_y], num_classes= 3))\n",
    "    print(\"Accuracy report for {}: {}\".format(categories, scores[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.69      0.69        13\n",
      "           0       0.62      0.67      0.64        24\n",
      "           1       0.72      0.67      0.69        27\n",
      "\n",
      "   micro avg       0.67      0.67      0.67        64\n",
      "   macro avg       0.68      0.68      0.67        64\n",
      "weighted avg       0.68      0.67      0.67        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.25      0.40        12\n",
      "           0       0.71      1.00      0.83        40\n",
      "           1       1.00      0.42      0.59        12\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        64\n",
      "   macro avg       0.90      0.56      0.61        64\n",
      "weighted avg       0.82      0.75      0.71        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.31      0.47        13\n",
      "           0       0.69      1.00      0.82        31\n",
      "           1       0.87      0.65      0.74        20\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        64\n",
      "   macro avg       0.85      0.65      0.68        64\n",
      "weighted avg       0.81      0.75      0.72        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', CountVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter= 300), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.69      0.69      0.69        13\n",
      "           0       0.72      0.54      0.62        24\n",
      "           1       0.67      0.81      0.73        27\n",
      "\n",
      "   micro avg       0.69      0.69      0.69        64\n",
      "   macro avg       0.69      0.68      0.68        64\n",
      "weighted avg       0.69      0.69      0.68        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        12\n",
      "           0       0.62      1.00      0.77        40\n",
      "           1       0.00      0.00      0.00        12\n",
      "\n",
      "   micro avg       0.62      0.62      0.62        64\n",
      "   macro avg       0.21      0.33      0.26        64\n",
      "weighted avg       0.39      0.62      0.48        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00        13\n",
      "           0       0.54      1.00      0.70        31\n",
      "           1       1.00      0.35      0.52        20\n",
      "\n",
      "   micro avg       0.59      0.59      0.59        64\n",
      "   macro avg       0.51      0.45      0.41        64\n",
      "weighted avg       0.58      0.59      0.50        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='lbfgs', max_iter= 300), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.75      0.69      0.72        13\n",
      "           0       0.69      0.75      0.72        24\n",
      "           1       0.73      0.70      0.72        27\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        64\n",
      "   macro avg       0.72      0.72      0.72        64\n",
      "weighted avg       0.72      0.72      0.72        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.25      0.40        12\n",
      "           0       0.73      1.00      0.84        40\n",
      "           1       1.00      0.50      0.67        12\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        64\n",
      "   macro avg       0.91      0.58      0.64        64\n",
      "weighted avg       0.83      0.77      0.73        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.54      0.70        13\n",
      "           0       0.75      0.97      0.85        31\n",
      "           1       0.82      0.70      0.76        20\n",
      "\n",
      "   micro avg       0.80      0.80      0.80        64\n",
      "   macro avg       0.86      0.74      0.77        64\n",
      "weighted avg       0.82      0.80      0.79        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', CountVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.92      0.75        13\n",
      "           0       0.71      0.62      0.67        24\n",
      "           1       0.75      0.67      0.71        27\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        64\n",
      "   macro avg       0.70      0.74      0.71        64\n",
      "weighted avg       0.71      0.70      0.70        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.25      0.40        12\n",
      "           0       0.73      1.00      0.84        40\n",
      "           1       1.00      0.50      0.67        12\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        64\n",
      "   macro avg       0.91      0.58      0.64        64\n",
      "weighted avg       0.83      0.77      0.73        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.38      0.53        13\n",
      "           0       0.71      0.97      0.82        31\n",
      "           1       0.81      0.65      0.72        20\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        64\n",
      "   macro avg       0.79      0.67      0.69        64\n",
      "weighted avg       0.77      0.75      0.73        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.61      0.85      0.71        13\n",
      "           0       0.71      0.62      0.67        24\n",
      "           1       0.80      0.74      0.77        27\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        64\n",
      "   macro avg       0.71      0.74      0.72        64\n",
      "weighted avg       0.73      0.72      0.72        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.33      0.50        12\n",
      "           0       0.74      1.00      0.85        40\n",
      "           1       1.00      0.50      0.67        12\n",
      "\n",
      "   micro avg       0.78      0.78      0.78        64\n",
      "   macro avg       0.91      0.61      0.67        64\n",
      "weighted avg       0.84      0.78      0.75        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.31      0.44        13\n",
      "           0       0.65      0.90      0.76        31\n",
      "           1       0.75      0.60      0.67        20\n",
      "\n",
      "   micro avg       0.69      0.69      0.69        64\n",
      "   macro avg       0.73      0.60      0.62        64\n",
      "weighted avg       0.71      0.69      0.67        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', CountVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.63      0.92      0.75        13\n",
      "           0       0.71      0.62      0.67        24\n",
      "           1       0.75      0.67      0.71        27\n",
      "\n",
      "   micro avg       0.70      0.70      0.70        64\n",
      "   macro avg       0.70      0.74      0.71        64\n",
      "weighted avg       0.71      0.70      0.70        64\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.25      0.40        12\n",
      "           0       0.73      1.00      0.84        40\n",
      "           1       1.00      0.50      0.67        12\n",
      "\n",
      "   micro avg       0.77      0.77      0.77        64\n",
      "   macro avg       0.91      0.58      0.64        64\n",
      "weighted avg       0.83      0.77      0.73        64\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.38      0.53        13\n",
      "           0       0.71      0.97      0.82        31\n",
      "           1       0.81      0.65      0.72        20\n",
      "\n",
      "   micro avg       0.75      0.75      0.75        64\n",
      "   macro avg       0.79      0.67      0.69        64\n",
      "weighted avg       0.77      0.75      0.73        64\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00         1\n",
      "           0       0.98      1.00      0.99        63\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        64\n",
      "   macro avg       0.49      0.50      0.50        64\n",
      "weighted avg       0.97      0.98      0.98        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
