{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formalizer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/formalizer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print('formalizer ' + str(response) + str(string))\n",
    "        return None\n",
    "    \n",
    "def stemmer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/stemmer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print('stemmer ' + str(response))\n",
    "        return None\n",
    "    \n",
    "def stopwords_removal(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/stopwords\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print('stopwords ' + str(response))\n",
    "        return None\n",
    "    \n",
    "def tokenizer(string):\n",
    "    req = requests.post(\"http://127.0.0.1:9000/sentence/tokenizer\", json= {\"string\": string})\n",
    "    response = req.json()\n",
    "    if response['status'] == 'success':\n",
    "        return response['data']\n",
    "    else:\n",
    "        print(response)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('label/review1.csv')\n",
    "data2 = pd.read_csv('label/review5.csv')\n",
    "data3 = pd.read_csv('label/review1_2.csv')\n",
    "data4 = pd.read_csv('label/review2_2.csv')\n",
    "data5 = pd.read_csv('label/review3_2.csv')\n",
    "data6 = pd.read_csv('label/review5_2.csv')\n",
    "data7 = pd.read_csv('label/review1_3.csv')\n",
    "data8 = pd.read_csv('label/review2_3.csv')\n",
    "data9 = pd.read_csv('label/review3_3.csv')\n",
    "data10 = pd.read_csv('label/review5_3.csv')\n",
    "\n",
    "datal = [data1, data2, data3, data4, data5, data6, data7, data8, data9, data10]\n",
    "data = pd.concat(datal)\n",
    "data = data[['review', 'produk', 'packaging', 'pengiriman', 'general']].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_aspect = []\n",
    "for d in data:\n",
    "    d_aspect = [d[0]] + [1 if aspect != 0 else 0 for aspect in d[1:]]\n",
    "    data_aspect.append(d_aspect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = []\n",
    "for d in data:\n",
    "    formalized_data = formalizer(d[0])\n",
    "    removed_data = stopwords_removal(formalized_data)\n",
    "    stemmed_data = stemmer(removed_data)\n",
    "    preprocessed_data.append(stemmed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d[1:] for d in data_aspect]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(preprocessed_data, y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.56      0.67        18\n",
      "           1       0.83      0.95      0.88        40\n",
      "\n",
      "   micro avg       0.83      0.83      0.83        58\n",
      "   macro avg       0.83      0.75      0.78        58\n",
      "weighted avg       0.83      0.83      0.82        58\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        43\n",
      "           1       0.83      0.67      0.74        15\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        58\n",
      "   macro avg       0.86      0.81      0.83        58\n",
      "weighted avg       0.88      0.88      0.87        58\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89        32\n",
      "           1       0.88      0.85      0.86        26\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        58\n",
      "   macro avg       0.88      0.88      0.88        58\n",
      "weighted avg       0.88      0.88      0.88        58\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        57\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.98      0.98      0.98        58\n",
      "   macro avg       0.49      0.50      0.50        58\n",
      "weighted avg       0.97      0.98      0.97        58\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', CountVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', LinearSVC())\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)\n",
    "    joblib.dump(pipeline, categories + \"_aspect.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df= 1)\n",
    "X = vectorizer.fit_transform(preprocessed_data).toarray()\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "tokenized = [list(map(lambda x: vectorizer.vocabulary_.get(x), analyzer(line))) for line in preprocessed_data]\n",
    "\n",
    "y = [d[1:] for d in data_aspect]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(tokenized, y, test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_words = 40\n",
    "train_x = sequence.pad_sequences(train_x, maxlen= max_words)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen= max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115/115 [==============================] - 0s 2ms/step\n",
      "Accuracy report for produk: 0.6608695610709813\n",
      "115/115 [==============================] - 0s 574us/step\n",
      "Accuracy report for packaging: 0.643478260351264\n",
      "115/115 [==============================] - 0s 557us/step\n",
      "Accuracy report for pengiriman: 0.5739130429599596\n",
      "115/115 [==============================] - 0s 538us/step\n",
      "Accuracy report for general: 0.756521737575531\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "embedding_size = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vectorizer.vocabulary_), embedding_size, input_length= max_words))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    model.fit(train_x, [t[i] for t in train_y], epochs= 16, verbose= 0)\n",
    "    scores = model.evaluate(test_x, [t[i] for t in test_y])\n",
    "    print(\"Accuracy report for {}: {}\".format(categories, scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Doc2Vec + SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [d[1:] for d in data_aspect]\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(preprocessed_data, y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for produk\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        14\n",
      "           1       0.56      1.00      0.72        18\n",
      "\n",
      "   micro avg       0.56      0.56      0.56        32\n",
      "   macro avg       0.28      0.50      0.36        32\n",
      "weighted avg       0.32      0.56      0.40        32\n",
      "\n",
      "Classification report for packaging\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84        23\n",
      "           1       0.00      0.00      0.00         9\n",
      "\n",
      "   micro avg       0.72      0.72      0.72        32\n",
      "   macro avg       0.36      0.50      0.42        32\n",
      "weighted avg       0.52      0.72      0.60        32\n",
      "\n",
      "Classification report for pengiriman\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.94      0.70        17\n",
      "           1       0.67      0.13      0.22        15\n",
      "\n",
      "   micro avg       0.56      0.56      0.56        32\n",
      "   macro avg       0.61      0.54      0.46        32\n",
      "weighted avg       0.61      0.56      0.47        32\n",
      "\n",
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        32\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        32\n",
      "   macro avg       1.00      1.00      1.00        32\n",
      "weighted avg       1.00      1.00      1.00        32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('clf', LogisticRegression())\n",
    "            ])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    pipeline.fit(train_x, [t[i] for t in train_y])\n",
    "    prediction = pipeline.predict(test_x)\n",
    "    print(\"Classification report for {}\".format(categories))\n",
    "    report = classification_report([t[i] for t in test_y], prediction)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "vocab_size, embedding_size = docvec_weight.shape\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim= vocab_size, output_dim= embedding_size, weights= [docvec_weight]))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "\n",
    "for i, categories in enumerate(['produk', 'packaging', 'pengiriman', 'general']):\n",
    "    model.fit(train_x, [t[i] for t in train_y], epochs= 16, verbose= 0)\n",
    "    scores = model.evaluate(test_x, [t[i] for t in test_y])\n",
    "    print(\"Accuracy report for {}: {}\".format(categories, scores[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization aspect for 'produk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "i = 1\n",
    "for d, s in zip(preprocessed_data, data):\n",
    "    if (s[i] != 0):\n",
    "        data_x.append(d)\n",
    "        data_y.append(1 if s[i] == 1 else 0)\n",
    "        \n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.85      0.88        13\n",
      "           1       0.93      0.96      0.95        27\n",
      "\n",
      "   micro avg       0.93      0.93      0.93        40\n",
      "   macro avg       0.92      0.90      0.91        40\n",
      "weighted avg       0.92      0.93      0.92        40\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['produk_sentiment.sav']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', LinearSVC())\n",
    "            ])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "prediction = pipeline.predict(test_x)\n",
    "print(\"Classification report for {}\".format(categories))\n",
    "report = classification_report(test_y, prediction)\n",
    "print(report)\n",
    "joblib.dump(pipeline, \"produk_sentiment.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization aspect for 'packaging'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "i = 2\n",
    "for d, s in zip(preprocessed_data, data):\n",
    "    if (s[i] != 0):\n",
    "        data_x.append(d)\n",
    "        data_y.append(1 if s[i] == 1 else 0)\n",
    "        \n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86         7\n",
      "           1       0.90      0.90      0.90        10\n",
      "\n",
      "   micro avg       0.88      0.88      0.88        17\n",
      "   macro avg       0.88      0.88      0.88        17\n",
      "weighted avg       0.88      0.88      0.88        17\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['packaging_sentiment.sav']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', LinearSVC())\n",
    "            ])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "prediction = pipeline.predict(test_x)\n",
    "print(\"Classification report for {}\".format(categories))\n",
    "report = classification_report(test_y, prediction)\n",
    "print(report)\n",
    "joblib.dump(pipeline, \"packaging_sentiment.sav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization aspect for 'pengiriman'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "i = 3\n",
    "for d, s in zip(preprocessed_data, data):\n",
    "    if (s[i] != 0):\n",
    "        data_x.append(d)\n",
    "        data_y.append(1 if s[i] == 1 else 0)\n",
    "        \n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for general\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80         8\n",
      "           1       0.88      0.93      0.90        15\n",
      "\n",
      "   micro avg       0.87      0.87      0.87        23\n",
      "   macro avg       0.87      0.84      0.85        23\n",
      "weighted avg       0.87      0.87      0.87        23\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pengiriman_sentiment.sav']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('vectorize', TfidfVectorizer(ngram_range= (1,2))),\n",
    "                ('clf', LinearSVC())\n",
    "            ])\n",
    "\n",
    "pipeline.fit(train_x, train_y)\n",
    "prediction = pipeline.predict(test_x)\n",
    "print(\"Classification report for {}\".format(categories))\n",
    "report = classification_report(test_y, prediction)\n",
    "print(report)\n",
    "joblib.dump(pipeline, \"pengiriman_sentiment.sav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
